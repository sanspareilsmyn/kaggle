{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Stacking을 이용한 Second-Level Learning Model"},{"metadata":{},"cell_type":"markdown","source":"안녕하세요. 취미로 캐글을 시작한 지 얼마 안 되어서 많이 부족한 커널이지만  \n즐겁게 만들어보았습니다! 자유로운 피드백은 늘 환영입니다!"},{"metadata":{},"cell_type":"markdown","source":"Stacking을 이용한 다음 모델은 이런 식으로 구현되어 있습니다.\n\n1. 5개의 First Level Classifier들이 (DecisionTree, RandomForest, ExtraTrees, AdaBoost, GBM) 각자 예측값을 0, 1의 형태로 도출한다.\n2. 이 예측값을 모아서 Second Level Classifier(XGBoost)에 넣어 최종 결과를 도출한다.\n\n**이 커널은 Baseline이며, 향후 Feature Engineering, Parameter Tuning 등을 통해 성능을 개선할 예정입니다.  \n피드백 및 성능 개선에 대한 아이디어는 무엇이든 감사하게 듣겠습니다!**"},{"metadata":{},"cell_type":"markdown","source":"# 1. 학습 가능한 형태로 데이터 변환"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom matplotlib impoty pyplot as plt\nimport seaborn as sns\n\nfrom category_encoders.ordinal import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import f1_score, accuracy_score\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/kakr-4th-competition/train.csv')\ntest_df = pd.read_csv('../input/kakr-4th-competition/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.drop(['id'], axis=1, inplace=True)\ntest_df.drop(['id'], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"우선 예측하고자 하는 'income' 을 True/False 형태로 변환해준 뒤, X와 y를 분리했습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"y = train_df['income'] != '<=50K'\nX = train_df.drop(['income'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ordinal Encoder를 이용한 라벨링을 진행합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"LE_encoder = OrdinalEncoder(list(X.columns))\n\nX = LE_encoder.fit_transform(X, y)\ntest_df = LE_encoder.transform(test_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"라벨링을 마치고 나면 아래와 같은 데이터로 정리됩니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"X['income'] = y\nX.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'native_country' 열만 float 형태여서, 다른 열과 동일하게 형변환을 진행했습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['native_country'] = test_df['native_country'].astype(np.int64)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 마지막으로 X_train, y_train, X_test를 나누어 저장해둡니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_train = X['income'].values\nX_train = X.drop(['income'], axis=1).values\nX_test = test_df.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. First-Level Classifier 만들기"},{"metadata":{},"cell_type":"markdown","source":"## 1) Model Creation"},{"metadata":{},"cell_type":"markdown","source":"Model_Creation이라는 클래스를 생성합니다.  \n5개의 Classifier를 효율적이고 빠르게 생성하기 위해 만드는 클래스입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"class Model_Creation(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n        \n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n        \n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self, x, y):\n        return self.clf.fit(x, y)\n    \n    def feature_importances(self, x, y):\n        print(self.clf.fit(x, y).feature_importances_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이번에는 get_scores 함수를 정의합니다.\nReturn값으로 4가지를 받습니다.  \n\n1. Training Prediction : X_train, y_train을 넣어줬을 때의 0, 1 예측값들입니다. 이 결과값은 추후에 합쳐진 뒤, Second-Level Classifier의 Input이 됩니다.  \n2. Test Prediction : X_test을 넣어줬을 때의 예측값입니다. First-Level Classifier들의 성능이 어떠한지 체크할 수 있습니다.\n3. First-level Accuracy Score : X_train, y_train의 Accuracy Score입니다.\n4. First-level F1 Score : X_train, y_train의 F1-Score입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_size = train_df.shape[0]\ntest_size = test_df.shape[0]\nSEED = 0\nNFOLDS = 5\nkf = KFold(n_splits=NFOLDS, random_state=SEED)\n\ndef get_scores(clf, x_train_get, y_train_get, x_test_get):\n    pred_train = np.zeros((train_size,))\n    pred_test = np.zeros((test_size,))\n    pred_test_kfold = np.empty((NFOLDS, test_size))\n        \n    for i, (train_index, val_index) in enumerate(kf.split(x_train_get)):\n        x_train = x_train_get[train_index]\n        y_train = y_train_get[train_index]\n        x_val = x_train_get[val_index]\n        \n        clf.train(x_train, y_train)\n        \n        pred_train[val_index] = clf.predict(x_val)\n        pred_test_kfold[i, :] = clf.predict(x_test_get)\n        \n    pred_test[:] = pred_test_kfold.mean(axis=0)\n    \n    pred_train = pred_train.astype(int)\n    pred_test = pred_test.astype(int)\n    \n    clf_acc_score = accuracy_score(y_train_get, pred_train)\n    clf_f1_score = f1_score(y_train_get, pred_train)\n    \n    return pred_train.reshape(-1, 1), pred_test.reshape(-1, 1), clf_acc_score, clf_f1_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"가장 기본적인 방식으로 Parameter들을 정의했습니다.  \n추후 이 값들은 Tuning이 필요합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_params = {\n    'max_depth' : 3,\n    'min_samples_split' : 2\n}\n\nada_params = {\n    'n_estimators': 100,\n    'learning_rate' : 0.75\n}\n\nrf_params = {\n    'n_estimators' : 100,\n    'min_samples_split' : 2\n}\n\net_params = {\n    'n_estimators': 100,\n    'min_samples_leaf': 2,\n}\n\ngb_params = {\n    'n_estimators': 100,\n    'min_samples_leaf': 2,\n}","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"5개의 모델을 아래와 같이 생성한 뒤, 결과값을 변수에 저장합니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_model = Model_Creation(clf=DecisionTreeClassifier, seed=SEED, params=dt_params)\nrf_model = Model_Creation(clf = RandomForestClassifier, seed = SEED, params = rf_params)\net_model = Model_Creation(clf = ExtraTreesClassifier, seed = SEED, params = et_params)\nada_model = Model_Creation(clf = AdaBoostClassifier, seed = SEED, params = ada_params)\ngb_model = Model_Creation(clf = GradientBoostingClassifier, seed = SEED, params = gb_params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_train_result, dt_test_result, dt_acc_score, dt_f1_score = get_scores(clf=dt_model, x_train_get=X_train, y_train_get=y_train, x_test_get=X_test)\nrf_train_result, rf_test_result, rf_acc_score, rf_f1_score = get_scores(clf=rf_model, x_train_get=X_train, y_train_get=y_train, x_test_get=X_test)\net_train_result, et_test_result, et_acc_score, et_f1_score = get_scores(clf=et_model, x_train_get=X_train, y_train_get=y_train, x_test_get=X_test)\nada_train_result, ada_test_result, ada_acc_score, ada_f1_score = get_scores(clf=ada_model, x_train_get=X_train, y_train_get=y_train, x_test_get=X_test)\ngb_train_result, gb_test_result, gb_acc_score, gb_f1_score = get_scores(clf=gb_model, x_train_get=X_train, y_train_get=y_train, x_test_get=X_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2) Performance & Feature Importance"},{"metadata":{},"cell_type":"markdown","source":"First-level Classifier들의 성능을 체크해봅시다."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Accuracy score of DecisionTreeClassifier :', round(dt_acc_score, 4) * 100, '%')\nprint('F1-Score of DecisionTreeClassifier :', round(dt_f1_score, 4) * 100)\n\nprint('Accuracy score of RandomForestClassifer :', round(rf_acc_score, 4) * 100, '%')\nprint('F1-Score of ExtraTreesClassifier :', round(rf_f1_score, 4) * 100)\n\nprint('Accuracy score of ExtraTreesClassifier :', round(et_acc_score, 4) * 100, '%')\nprint('F1-Score of ExtraTreesClassifier :', round(et_f1_score, 4) * 100)\n\nprint('Accuracy score of AdaBoost :', round(ada_acc_score, 4) * 100, '%')\nprint('F1-Score of AdaBoost :', round(ada_f1_score, 4) * 100)\n\nprint('Accuracy score of Gradient Boosting Machine :', round(gb_acc_score, 4) * 100, '%')\nprint('F1-Score of Gradient Boosting Machine :', round(gb_f1_score, 4) * 100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First-level Classifier들은 어떤 Feature에 가중치를 두고 학습되었는지 확인해봅시다."},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_features = dt_model.feature_importances(X_train, y_train)\nrf_features = rf_model.feature_importances(X_train, y_train)\net_features = et_model.feature_importances(X_train, y_train)\nada_features = ada_model.feature_importances(X_train, y_train)\ngb_features = gb_model.feature_importances(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dt_features = [0.00158387, 0, 0, 0, 0.21968882, 0.52720064, 0, 0, 0, 0, 0.25152667, 0, 0, 0]\n\nrf_features = [0.14884607, 0.03715319, 0.1672759,  0.0335612, 0.08463101, 0.09981356,\n 0.07028089, 0.07831672, 0.0139579, 0.01390901, 0.11619356, 0.03545653,\n 0.08397304, 0.01663142]\n\net_features = [0.09821158, 0.03551721, 0.03277514, 0.05533726, 0.14090152, 0.13328737,\n 0.05471377, 0.14526666, 0.01215226, 0.04887652, 0.13526527, 0.0378438,\n 0.06095664, 0.00889499]\n\nada_features = [0.11, 0.02, 0.02, 0.01, 0.07, 0.06, 0.17, 0.08, 0.01, 0.04, 0.21, 0.17, 0.03, 0]\n\ngb_features = [0.06090052, 0.00509677, 0.00435494, 0.00256431, 0.19704072, 0.38849472,\n 0.02393962, 0.00597061, 0.00107229, 0.00193148, 0.20452907, 0.06179946,\n 0.04085863, 0.00144688]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_tmp = X.drop(['income'], axis=1)\nX_tmp.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df = pd.DataFrame({\n    'Features' : X_tmp.columns.values,\n    'DT_feat_importances_' : dt_features,\n    'RF_feat_importances_' : rf_features,\n    'ET_feat_importances_' : et_features,\n    'ADA_feat_importances_' : ada_features,\n    'GB_feat_importances_' : gb_features\n})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"feature_df에 각 Classifier들의 importance를 모아놓았습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_df.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이를 Seaborn을 이용해서 그림으로 확인해봅시다."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_feature_importance(clf_features):\n    plt.figure(figsize = (20, 12))\n    feat_plot = sns.barplot(data=feature_df, x='Features', y=clf_features)\n    feat_plot.set_title(clf_features, fontdict={'fontsize' : 20})\n    for p in feat_plot.patches:\n        feat_plot.annotate(format(p.get_height(), '.2f'),\n                        (p.get_x() + p.get_width() / 2., p.get_height()),\n                         ha = 'center', va = 'center',\n                         textcoords = 'offset points')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance('DT_feat_importances_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance('RF_feat_importances_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance('ET_feat_importances_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance('ADA_feat_importances_')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_feature_importance('GB_feat_importances_')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 3. Second-Level Classifier 만들기"},{"metadata":{"trusted":true},"cell_type":"code","source":"first_level_pred = pd.DataFrame({\n    'DecisionTree' : dt_train_result.ravel(),\n    'RandomForest' : rf_train_result.ravel(),\n    'ExtraTrees' : et_train_result.ravel(),\n    'AdaBoost' : ada_train_result.ravel(),\n    'GBM' : gb_train_result.ravel()\n    \n})\nfirst_level_pred.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"아래는 5개 Classifier들의 Consensus를 보여주는 값입니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"first_level_pred.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"이제 이 예측값들을 하나로 묶어서 Second-level Classifier에 넣습니다."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_secondlevel = np.concatenate((dt_train_result, rf_train_result, et_train_result,\n                                     ada_train_result, gb_train_result), axis=1)\nX_test_secondlevel = np.concatenate((dt_test_result, rf_test_result, et_test_result,\n                                     ada_test_result, gb_test_result), axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_model = xgb.XGBClassifier(\n    n_estimators = 100,\n    max_depth = 4,\n    min_child_weight = 2,\n    objective = 'binary:logistic').fit(X_train_secondlevel, y_train)\nfinal_pred = xgb_model.predict(X_test_secondlevel)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission = pd.read_csv('/kaggle/input/kakr-4th-competition/sample_submission.csv')\nsample_submission['prediction'] = final_pred.astype(int)\nsample_submission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Reference"},{"metadata":{},"cell_type":"markdown","source":"1. [[KaKr] 탐색적 데이터 분석(EDA) 설명 + 예시](https://www.kaggle.com/subinium/kakr-eda)  \n2. [캐하~ EDA + LightGBM + PyCaret](https://www.kaggle.com/teddylee777/eda-lightgbm-pycaret)  \n3. [Introduction to Ensembling/Stacking in Python](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}